{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b422f388",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ldt_session_1_data_transformed.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mldt_session_1_data_transformed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m ldt_s1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mldt_session_1_data_transformed.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m ldt_s2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mldt_session_2_data_transformed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m rmt_s1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmt_session_1_data_transformed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/m_base/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/m_base/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/m_base/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/m_base/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/m_base/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/m_base/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/m_base/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ldt_session_1_data_transformed.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'ldt_session_1_data_transformed.csv'\n",
    "ldt_s1 = pd.read_csv('ldt_session_1_data_transformed.csv').sort_values(by=['ID'], ascending=False)\n",
    "ldt_s2 = pd.read_csv('ldt_session_2_data_transformed.csv').sort_values(by=['ID'], ascending=False)\n",
    "rmt_s1 = pd.read_csv('rmt_session_1_data_transformed.csv').sort_values(by=['ID'], ascending=False)\n",
    "rmt_s2 = pd.read_csv('rmt_session_2_data_transformed.csv').sort_values(by=['ID'], ascending=False)\n",
    "ldt_s1=ldt_s1[ldt_s1['ID'].isin(rmt_s1['ID'])]\n",
    "ldt_s2=ldt_s2[ldt_s2['ID'].isin(rmt_s1['ID'])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34e971-431f-4e19-b418-151e79235655",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldt_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c68298-e38b-4987-8da2-af93001dbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = ldt_s1 + ldt_s2 + rmt_s1 + rmt_s2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ab490-43ef-4770-9de2-7b0b161999dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53e065-65e9-4608-be55-367df3874e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave=sum[['v1_mean',\n",
    " 'v2_mean',\n",
    " 'zr_mean',\n",
    " 'a_mean',\n",
    " 'ndt_mean',\n",
    " 'sndt_mean',\n",
    " 'alpha_mean',\n",
    " 'mean_log_rt',\n",
    " 'alpha_mean_boxcox_after_arcsin',\n",
    " 'accuracy_percentage_boxcox_after_arcsin'\n",
    "                            ]]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a75fb-0f9c-41ec-8791-796ce1047465",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave.insert(loc=0, column='ID', value=ldt_s1['ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3bb0e-7a3d-4e9f-91e0-3935050a4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2608e0c-48a3-4266-8f1f-9fc56ee0734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c810e5-ee27-4e8f-b52b-e51688c76844",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1883ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define the safe Mahalanobis function\n",
    "def safe_mahalanobis(x=None, data=None, cov=None):\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "# Let's apply this to the first pair of parameters and check the result\n",
    "pair = ['v1_mean', 'v2_mean']\n",
    "data_pair = data[pair]\n",
    "\n",
    "# Calculating Mahalanobis distance for the pair\n",
    "mahal_distances = safe_mahalanobis(x=data_pair, data=data_pair)\n",
    "\n",
    "# Determine the p-value threshold for outlier detection (p < 0.001)\n",
    "p_value_threshold = 0.001\n",
    "\n",
    "# Calculate the critical value for this p-value under the chi-squared distribution\n",
    "critical_value = stats.chi2.ppf((1 - p_value_threshold), df=2)\n",
    "\n",
    "# Identifying outliers\n",
    "outliers = mahal_distances > critical_value\n",
    "\n",
    "# Display the first few Mahalanobis distances and the outlier flags\n",
    "mahal_distances[:5], outliers[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define the function for Mahalanobis distance\n",
    "def safe_mahalanobis(x=None, data=None, cov=None):\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "# Select the first pair of parameters to demonstrate the process\n",
    "params = ['v1_mean', 'v2_mean']\n",
    "data_subset = data[params]\n",
    "\n",
    "# Calculate the Mahalanobis distance for each observation\n",
    "mahal_distances = safe_mahalanobis(x=data_subset, data=data_subset)\n",
    "\n",
    "# Determine the threshold for outliers using the Chi-square distribution\n",
    "p_value = 0.001\n",
    "threshold = stats.chi2.ppf((1 - p_value), df=2)  # df is the number of variables, here 2\n",
    "\n",
    "# Identify outliers\n",
    "outliers = mahal_distances > threshold\n",
    "\n",
    "# Print the number of outliers detected\n",
    "num_outliers = np.sum(outliers)\n",
    "num_outliers, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define the function for Mahalanobis distance\n",
    "def safe_mahalanobis(x=None, data=None, cov=None):\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "# Select the first pair of parameters to demonstrate the process\n",
    "params = ['v1_mean', 'v2_mean']\n",
    "data_subset = data[params]\n",
    "\n",
    "# Calculate the Mahalanobis distance for each observation\n",
    "mahal_distances = safe_mahalanobis(x=data_subset, data=data_subset)\n",
    "\n",
    "# Determine the threshold for outliers using the Chi-square distribution\n",
    "p_value = 0.001\n",
    "threshold = stats.chi2.ppf((1 - p_value), df=2)  # df is the number of variables, here 2\n",
    "\n",
    "# Identify outliers\n",
    "outliers = mahal_distances > threshold\n",
    "\n",
    "# Print the number of outliers detected\n",
    "num_outliers = np.sum(outliers)\n",
    "num_outliers, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove outliers for the first pair\n",
    "data_no_outliers = data_subset[~outliers]\n",
    "\n",
    "# Calculate correlation for the first pair without outliers\n",
    "correlation = data_no_outliers.corr().iloc[0, 1]\n",
    "\n",
    "# Plotting to show the process\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot with outliers\n",
    "sns.scatterplot(data=data_subset, x=params[0], y=params[1], ax=ax[0])\n",
    "ax[0].set_title(f'Scatter Plot with Outliers\\n({params[0]} vs {params[1]})')\n",
    "\n",
    "# Scatter plot without outliers\n",
    "sns.scatterplot(data=data_no_outliers, x=params[0], y=params[1], ax=ax[1])\n",
    "ax[1].set_title(f'Scatter Plot without Outliers\\n({params[0]} vs {params[1]})\\nCorrelation: {correlation:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# List of all selected parameters\n",
    "selected_params = ['v1_mean', 'v2_mean', 'zr_mean', 'a_mean', 'ndt_mean', 'sndt_mean', 'alpha_mean', \n",
    "                   'alpha_mean_boxcox_after_arcsin', 'mean_log_rt', 'accuracy_percentage_boxcox_after_arcsin']\n",
    "\n",
    "# Preparing a DataFrame to store correlations\n",
    "correlation_matrix = pd.DataFrame(index=selected_params, columns=selected_params)\n",
    "\n",
    "# Calculate Mahalanobis distance and correlations for each pair of parameters\n",
    "for pair in combinations(selected_params, 2):\n",
    "    pair_data = data[list(pair)]\n",
    "    mahal_distances = safe_mahalanobis(x=pair_data, data=pair_data)\n",
    "    outliers = mahal_distances > threshold\n",
    "    correlation = pair_data[~outliers].corr().iloc[0, 1]\n",
    "    correlation_matrix.loc[pair[0], pair[1]] = correlation\n",
    "    correlation_matrix.loc[pair[1], pair[0]] = correlation  # Mirror the matrix\n",
    "\n",
    "# Fill diagonal with NaN as we will put histograms there\n",
    "np.fill_diagonal(correlation_matrix.values, np.nan)\n",
    "#correlation_matrix_renamed.to_csv('LDT_s1_correlation_matrix_renamed.csv')\n",
    "correlation_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683af827-6a3e-4523-9d25-6c88df97cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in combinations(selected_params, 2):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620877c-c117-4924-bae7-4862fe674f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Function to calculate Mahalanobis distance\n",
    "def safe_mahalanobis(x=None, data=None, cov=None):\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "# Load your data\n",
    "#data = pd.read_csv('ldt_session_1_data_transformed.csv')  # Replace with your file path\n",
    "\n",
    "# Define your parameters\n",
    "selected_params = ['v1_mean', 'v2_mean', 'zr_mean', 'a_mean', 'ndt_mean', 'sndt_mean', \n",
    "                   'alpha_mean', 'alpha_mean_boxcox_after_arcsin', \n",
    "                   'mean_log_rt', 'accuracy_percentage_boxcox_after_arcsin']\n",
    "\n",
    "# Renaming for simplicity\n",
    "renamed_params = {\n",
    "    'v1_mean': 'v1', 'v2_mean': 'v2', 'zr_mean': 'zr', 'a_mean': 'a', \n",
    "    'ndt_mean': 'ndt', 'sndt_mean': 'sndt', 'alpha_mean': 'alpha', \n",
    "    'alpha_mean_boxcox_after_arcsin': 'alpha_arc_box', \n",
    "    'mean_log_rt': 'mean_log_rt', \n",
    "    'accuracy_percentage_boxcox_after_arcsin': 'acc_arc_box'\n",
    "}\n",
    "data_renamed = data.rename(columns=renamed_params)\n",
    "\n",
    "# Prepare for outlier detection and correlation\n",
    "correlation_matrix = pd.DataFrame(index=selected_params, columns=selected_params)\n",
    "p_value = 0.001\n",
    "\n",
    "# Process each pair of parameters\n",
    "for pair in combinations(selected_params, 2):\n",
    "    pair_data = data[list(pair)]\n",
    "    mahal_distances = safe_mahalanobis(x=pair_data, data=pair_data)\n",
    "    threshold = stats.chi2.ppf((1 - p_value), df=2)\n",
    "    outliers = mahal_distances > threshold\n",
    "    correlation = pair_data[~outliers].corr().iloc[0, 1]\n",
    "    correlation_matrix.loc[pair[0], pair[1]] = correlation\n",
    "    correlation_matrix.loc[pair[1], pair[0]] = correlation\n",
    "\n",
    "np.fill_diagonal(correlation_matrix.values, np.nan)\n",
    "correlation_matrix_renamed = correlation_matrix.rename(columns=renamed_params, index=renamed_params)\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_results = {}\n",
    "for param in selected_params:\n",
    "    shapiro_test = stats.shapiro(data[param])\n",
    "    shapiro_results[param] = shapiro_test[1]\n",
    "shapiro_results_renamed = {renamed_params[key]: value for key, value in shapiro_results.items()}\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, param_i in enumerate(renamed_params.values()):\n",
    "    for j, param_j in enumerate(renamed_params.values()):\n",
    "        plt.subplot(len(renamed_params), len(renamed_params), i*len(renamed_params) + j + 1)\n",
    "        if i == j:\n",
    "            sns.histplot(data_renamed[param_i], kde=True)\n",
    "            plt.title(f'{param_i}\\nShapiro p={shapiro_results_renamed[param_i]:.2e}')\n",
    "        elif i > j:\n",
    "            scatter_plot = sns.scatterplot(data=data_renamed, x=param_j, y=param_i, hue=outliers, legend=False)\n",
    "            if i == j + 1:\n",
    "                scatter_plot.legend(loc='upper right', title='Outlier', prop={'size': 6})\n",
    "            plt.title(f'Scatter Plot\\n({param_j} vs {param_i})')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, f'Corr: {correlation_matrix_renamed.iloc[i, j]:.2f}', \n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "            plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_par_ave_in_subject_level_distinct_correlation_Plot_renamed.png')  # Saves the plot to a file\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e4b18-852f-43a1-becd-f193d933ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_renamed = correlation_matrix_renamed.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad824370-bf58-49f0-89f1-88f7510c6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_renamed = correlation_matrix_renamed.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2ddf6-87e7-46da-8981-b257e4447ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a1cc6-bd02-4b8b-9130-b27b9782429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_renamed.to_csv('all_par_ave_in_subject_level_distinct_correlation_matrix_renamed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276c335-2331-4f35-afd5-ce29d04ff89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Function to calculate Mahalanobis distance\n",
    "def safe_mahalanobis(x=None, data=None, cov=None):\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "# Load your data\n",
    "#data = pd.read_csv('ldt_session_1_data_transformed.csv')  # Replace with your file path\n",
    "\n",
    "# Define your parameters\n",
    "selected_params = ['v1_mean', 'v2_mean', 'zr_mean', 'a_mean', 'ndt_mean',\n",
    "                  # 'sndt_mean', 'alpha_mean',\n",
    "                   'alpha_mean_boxcox_after_arcsin', \n",
    "                   'mean_log_rt', 'accuracy_percentage_boxcox_after_arcsin']\n",
    "\n",
    "# Renaming for simplicity\n",
    "renamed_params = {\n",
    "    'v1_mean': 'v1', 'v2_mean': 'v2', 'zr_mean': 'zr', 'a_mean': 'a', \n",
    "    'ndt_mean': 'ndt', #'sndt_mean': 'sndt',# 'alpha_mean': 'alpha', \n",
    "    'alpha_mean_boxcox_after_arcsin': 'alpha_arc_box', \n",
    "    'mean_log_rt': 'mean_log_rt', \n",
    "    'accuracy_percentage_boxcox_after_arcsin': 'acc_arc_box'\n",
    "}\n",
    "data_renamed = data.rename(columns=renamed_params)\n",
    "\n",
    "# Prepare for outlier detection and correlation\n",
    "correlation_matrix = pd.DataFrame(index=selected_params, columns=selected_params)\n",
    "p_value = 0.001\n",
    "\n",
    "# Process each pair of parameters\n",
    "for pair in combinations(selected_params, 2):\n",
    "    pair_data = data[list(pair)]\n",
    "    mahal_distances = safe_mahalanobis(x=pair_data, data=pair_data)\n",
    "    threshold = stats.chi2.ppf((1 - p_value), df=2)\n",
    "    outliers = mahal_distances > threshold\n",
    "    correlation = pair_data[~outliers].corr().iloc[0, 1]\n",
    "    correlation_matrix.loc[pair[0], pair[1]] = correlation\n",
    "    correlation_matrix.loc[pair[1], pair[0]] = correlation\n",
    "\n",
    "np.fill_diagonal(correlation_matrix.values, np.nan)\n",
    "correlation_matrix_renamed = correlation_matrix.rename(columns=renamed_params, index=renamed_params)\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_results = {}\n",
    "for param in selected_params:\n",
    "    shapiro_test = stats.shapiro(data[param])\n",
    "    shapiro_results[param] = shapiro_test[1]\n",
    "shapiro_results_renamed = {renamed_params[key]: value for key, value in shapiro_results.items()}\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, param_i in enumerate(renamed_params.values()):\n",
    "    for j, param_j in enumerate(renamed_params.values()):\n",
    "        plt.subplot(len(renamed_params), len(renamed_params), i*len(renamed_params) + j + 1)\n",
    "        if i == j:\n",
    "            sns.histplot(data_renamed[param_i], kde=True)\n",
    "            plt.title(f'{param_i}\\nShapiro p={shapiro_results_renamed[param_i]:.2e}')\n",
    "        elif i > j:\n",
    "            scatter_plot = sns.scatterplot(data=data_renamed, x=param_j, y=param_i, hue=outliers, legend=False)\n",
    "            if i == j + 1:\n",
    "                scatter_plot.legend(loc='upper right', title='Outlier', prop={'size': 6})\n",
    "            #plt.title(f'Scatter Plot\\n({param_j} vs {param_i})')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, f'Corr: {correlation_matrix_renamed.iloc[i, j]:.2f}', \n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "            plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('LDT_s1_exclude_sndt_alphaMean_correlation_matrix_plot.png')  # Saves the plot to a file\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b510a-ec33-41ff-8567-04c0a4e79c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Function to calculate Mahalanobis distance\n",
    "def safe_mahalanobis(x=None, data=None, cov=None):\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('ldt_session_1_data_transformed.csv')  # Replace with your file path\n",
    "\n",
    "# Define your parameters\n",
    "selected_params = ['v1_mean', 'v2_mean',# 'zr_mean', \n",
    "                   'a_mean', 'ndt_mean',\n",
    "                  # 'sndt_mean', 'alpha_mean',\n",
    "                   'alpha_mean_boxcox_after_arcsin', \n",
    "                   'mean_log_rt', 'accuracy_percentage_boxcox_after_arcsin']\n",
    "\n",
    "# Renaming for simplicity\n",
    "renamed_params = {\n",
    "    'v1_mean': 'v1', 'v2_mean': 'v2',# 'zr_mean': 'zr', \n",
    "    'a_mean': 'a', \n",
    "    'ndt_mean': 'ndt', #'sndt_mean': 'sndt',# 'alpha_mean': 'alpha', \n",
    "    'alpha_mean_boxcox_after_arcsin': 'alpha_arc_box', \n",
    "    'mean_log_rt': 'mean_log_rt', \n",
    "    'accuracy_percentage_boxcox_after_arcsin': 'acc_arc_box'\n",
    "}\n",
    "#data_renamed = data.rename(columns=renamed_params)\n",
    "\n",
    "# Prepare for outlier detection and correlation\n",
    "correlation_matrix = pd.DataFrame(index=selected_params, columns=selected_params)\n",
    "p_value = 0.001\n",
    "\n",
    "# Process each pair of parameters\n",
    "for pair in combinations(selected_params, 2):\n",
    "    pair_data = data[list(pair)]\n",
    "    mahal_distances = safe_mahalanobis(x=pair_data, data=pair_data)\n",
    "    threshold = stats.chi2.ppf((1 - p_value), df=2)\n",
    "    outliers = mahal_distances > threshold\n",
    "    correlation = pair_data[~outliers].corr().iloc[0, 1]\n",
    "    correlation_matrix.loc[pair[0], pair[1]] = correlation\n",
    "    correlation_matrix.loc[pair[1], pair[0]] = correlation\n",
    "\n",
    "np.fill_diagonal(correlation_matrix.values, np.nan)\n",
    "correlation_matrix_renamed = correlation_matrix.rename(columns=renamed_params, index=renamed_params)\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_results = {}\n",
    "for param in selected_params:\n",
    "    shapiro_test = stats.shapiro(data[param])\n",
    "    shapiro_results[param] = shapiro_test[1]\n",
    "shapiro_results_renamed = {renamed_params[key]: value for key, value in shapiro_results.items()}\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, param_i in enumerate(renamed_params.values()):\n",
    "    for j, param_j in enumerate(renamed_params.values()):\n",
    "        plt.subplot(len(renamed_params), len(renamed_params), i*len(renamed_params) + j + 1)\n",
    "        if i == j:\n",
    "            sns.histplot(data_renamed[param_i], kde=True)\n",
    "            plt.title(f'{param_i}\\nShapiro p={shapiro_results_renamed[param_i]:.2e}')\n",
    "        elif i > j:\n",
    "            scatter_plot = sns.scatterplot(data=data_renamed, x=param_j, y=param_i, hue=outliers, legend=False)\n",
    "            if i == j + 1:\n",
    "                scatter_plot.legend(loc='upper right', title='Outlier', prop={'size': 6})\n",
    "            #plt.title(f'Scatter Plot\\n({param_j} vs {param_i})')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, f'Corr: {correlation_matrix_renamed.iloc[i, j]:.2f}', \n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "            plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('LDT_S1_exclude_sndt_alphaMean_zr_correlation_matrix_plot.png')  # Saves the plot to a file\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3a1ff-2932-400b-b16f-0c82dcbde5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
